# CSE547 - Colab 2
# Frequent Pattern Mining in Spark
# Adapted from Stanford CS246
# Setup
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import logging
import pyspark
from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark import SparkContext, SparkConf
from pyspark.sql import functions as F

# Let's initialize the Spark context.
# create the session
conf = SparkConf().set("spark.ui.port", "4050")

# create the context
sc = pyspark.SparkContext(conf=conf)
spark = SparkSession.builder.getOrCreate()

# Your task!
products = spark.read.csv('products.csv', header=True, inferSchema=True)
orders = spark.read.csv('order_products__train.csv', header=True, inferSchema=True)

# If you run successfully the setup stage, you are ready to work with the 3 Million Instacart Orders dataset. In case you want to read more about it, check the official Instacart blog post about it, a concise schema description of the dataset, and the download page.
# In this Colab, we will be working only with a small training dataset (~131K orders) to perform fast Frequent Pattern Mining with the FP-Growth algorithm.
products = spark.read.csv('products.csv', header=True, inferSchema=True)
orders = spark.read.csv('order_products__train.csv', header=True, inferSchema=True)

# Step 1: Look at the products schema
p = products._jdf.schema().treeString()
o = orders._jdf.schema().treeString()
logging.info("Step 1: product.printSchema: {}".format(p))
logging.info("Step 1: orders.printSchema: {}".format(p))
# You can also show the schema by product.printSchema() function but the output is not a string.

## Your code!!!
# Use the Spark Dataframe API to join 'products' and 'orders', so that you will be able to see the product names in each transaction (and not only their ids). Then, group by the orders by 'order_id' to obtain one row per basket (i.e., set of products purchased together by one customer).
# YOUR CODE HERE

# In this Colab we will explore MLlib, Apache Spark's scalable machine learning library. Specifically, you can use its implementation of the FP-Growth algorithm to perform efficiently Frequent Pattern Mining in Spark. Use the Python example in the documentation, and train a model with
# minSupport=0.01 and minConfidence=0.5
# You can follow the URL here: https://spark.apache.org/docs/latest/ml-frequent-pattern-mining.html#fp-growth
# YOUR CODE HERE

# Compute how many Frequent Itemsets and Association Rules were generated by running FP-growth.
# YOUR CODE HERE

# Now, for minsupport=0.001, visualize the top Frequent Itemsets and Association Rules.
# In addition, for minsupport=0.001, calculate the "interest" of the Association Rules.
# (Hint: One way to do this is by joining the Frequent Itemsets with the Association Rules, then introducing an "interest" column using the joined dataframe)
# YOUR CODE HERE